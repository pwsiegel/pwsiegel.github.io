---
layout: post
permalink: /ds/entropy-defn/
title: On the Definition of Shannon Entropy
abstract: The definition of Shannon entropy admits a variety of appealing characterizations; here I will explore the characterization via "average surprisal".
level: hard
date: 2017-03-05
categories: [math, data-science]
tags: live
comments: true
---

In a [previous post][4] I went through some of the key ideas in Shannon's landmark paper *A Mathematical Theory of Communication*.  Perhaps the most enduring innovation in that paper is Shannon's definition of the *entropy* of a random variable which Shannon used as a measure of how much information the random variable produces.

These days information theory is a cornerstone of pure and applied mathematics, and as such its foundations have been examined quite carefully.  In my post above I introduced a description of Shannon entropy (due, I believe, to Myron Tribus) as a measure of how "surprising" an event generated by a random variable is on average; in this post I will examine this description more rigorously.

## Surprise!

In what follows we will use the notation $(\Omega, \Sigma, \P)$ for probability spaces, so that $(\Omega, \Sigma)$ is a measurable space equipped with a probability measure $\P$.  We will write $\R^\pm$ for the compactification of $\R$ with points at $\pm \infty$ adjoined.

<div class="definition">
A *surprise function* on a probability space $(\Omega, \Sigma, \P)$ is a function $S \colon \Sigma \to \R^\pm$ with the following properties:

1. $S(E_1) = S(E_2)$ whenever $\P(E_1) = \P(E_2)$.
2. $S(E_1) < S(E_2)$ whenever $\P(E_1) > \P(E_2)$.
3. If $E_1$ and $E_2$ are independent events then $S(E_1 \cap E_2) = S(E_1) + S(E_2)$.
</div>

In plain language: the first condition says that surprisal of an event depends only on its probability; the second condition says that low probability events are more surprising than high probability events; and the third condition says that surprise is additive when two independent events occur simultaneously.

There is a standard example of a surprise function:

<div class="lemma">
Let $(\Omega, \Sigma, \P)$ be a probability space.  Then the function $S \colon \Sigma \to \R^\pm$ given by:

$$S(E) = -\log \P(E)$$

(The base of the logarithm can be any positive real number; changing the base scales $S$ by a positive constant.  We define $\log 0 = -\infty$.)
</div>
<div class="proof">
Condition 1 in the definition of surprise function is obvious from the formula for $S$.  Condition 2 follows from the fact that the logarithm function is strictly increasing.  To prove condition 3, recall that $\P(E_1 \cap E_2) = \P(E_1)\P(E_2)$ whenever $E_1$ and $E_2$ are independent (by definition), so we have:

$$
\begin{align*}
S(E_1 \cap E_2) &= -C \log \P(E_1 \cap E_2) \\
&= -C \log \P(E_1) \P(E_2) \\
&= -C \log \P(E_1) - C \log \P(E_2) \\
&= S(E_1) + S(E_2)
\end{align*}
$$

</div>

## Characterizing Surprise

The function $S(E) = -\log \P(E)$ is by no means the only surprise function on most probability spaces.  Consider, for instance, the probability space corresponding to a biased coin flip: $\Omega = \br{H, T}$, $\Sigma = \br{\emptyset, \br{H}, \br{T}, \Omega}$, and $\P(\br{H}) = \frac{1}{4}$, $\P(\br{T}) = \frac{3}{4}$.  The events $\br{H}$ and $\br{T}$ are each independent of $\emptyset$ and $\Omega$, and this forces $S(\emptyset) = \infty$ and $S(\Omega) = 0$ (see below).  But there are no additional pairs of independent events to further constrain $S$, and indeed we are free to assign any pair of positive real values to $S(\br{H})$ and $S(\br{T})$ so long as $S(\br{H}) > S(\br{T})$.

But $S(E) = -\log \P(E)$ is in a sense the unique *universal* surprise function.  To express this properly, note that the first axiom in the definition of surprise asserts that $S(E) = f(\P(E))$ for some function $f \colon [0,1] \to \R^\pm$, and what we showed in the previous section was that the function $f(x) = -\log x$ does the job for any probability space.  We will now prove that it is the only such function.

<div class="proposition">
Suppose $f \colon [0,1] \to \R^\pm$ is a function with the property that $S(E) = f(\P(E))$ is a surprise function on any probability space $(\Omega, \Sigma, \P)$.  Then $f(x) = -\log x$.
</div>
<div class="proof">
First, let us show that $f$ is decreasing.  Take $a_1$ and $a_2$ in $[0,1]$ with $a_1 < a_2$.  Set $p_1 = \frac{a_1}{2}$ and $p_2 = \frac{a_2}{2}$; we have $0 \leq p_1, p_2 \leq \frac{1}{2}$.  Consider the probability space with $\Omega = \br{\omega_1, \ldots, \omega_5}$ and probabilities:

$$
\begin{align*}
\P(\br{\omega_1}) = \P(\br{\omega_2}) &= p_1 \\
\P(\br{\omega_3}) = \P(\br{\omega_4}) &= p_2 \\
\P(\br{\omega_5}) &= 1 - p_1 - p_2
\end{align*}
$$

Then $\P(\br{\omega_1, \omega_2}) = a_1$ and $\P(\br{\omega_3, \omega_4}) = a_2$, so we have:

$$f(a_1) = S(\br{\omega_1, \omega_2}) > S(\br{\omega_3, \omega_4}) = f(a_2)$$

since $S(E) = f(\P(E))$ is by assumption a surprise function.

Second, let us show that $f$ satisfies the identity:

$$
\begin{equation} \label{funceqn}
f(a_1 a_2) = f(a_1) + f(a_2)
\end{equation}
$$

for all real numbers $a_1$ and $a_2$ in $[0,1]$.  Let $(\Omega_1, \Sigma_1, \P_1)$ denote the biased coin flip probability space with $\Omega_1 = \br{H_1, T_1}$ and probabilities $\P_1(H_1) = a_1$, $\P_1(T_1) = 1 - a_1$.  Define $(\Omega_2, \Sigma_2, \P_2)$ similarly but with $\P_2(H_2) = a_2$.  Finally define $(\Omega, \Sigma, \P)$ to be the product of these two probability spaces; in this space the event $H_1$ is independent of the event $H_2$, so we have:

$$
\begin{align*}
f(a_1) + f(a_2) &= S(\br{H_1 H_2, H_1 T_2}) + S(\br{H_1 H_2, T_1 H_2}) \\
&= S(\br{H_1 H_2}) \\
&= f(a_1 a_2)
\end{align*}
$$

Thus $f(x)$ is a decreasing function which satisfies the functional equation \eqref{funceqn}.  It is a standard fact from analysis that $f(x) = -\log x$ is the only function with these properties; see, for instance, page 81 of [Dieudonne's book][1].
</div>

## Entropy and Surprise

Let $(\Omega, \Sigma, \P)$ be a discrete probability space, and consider the function $\Omega \to \R$ given by $\omega \mapsto -\log \P(\omega)$.  This function defines a random variable which we will call $I$ for *information* (though it is really just the surprise function described above), and we define the *entropy* of the probability space to be:

$$
\begin{equation} \label{entropy}
H = \E(I) = -\sum_{\omega \in \Omega} \P(\omega) \log \P(\omega)
\end{equation}
$$

More generally if $X$ is an $(\Omega, \Sigma)$-valued random variable on $(\Omega, \Sigma, \P)$ then we may define a new random variable $I(X)$ by:

$$I(X)(\omega) = -\log \P(X = \omega) = -\log f(\omega)$$

where $f$ is the probability density function of $X$.  We define the entropy of $X$ to be:

$$
\begin{equation} \label{entropyRV}
H(X) = \E(I(X)) = -\sum_{\omega \in \Omega} f(\omega) \log f(\omega)
\end{equation}
$$

But what about continuous probability spaces?  It is tempting to use \eqref{entropyRV} as a guide and define the entropy of a continuous random variable $X$ by:

$$H(X) = -\int_{-\infty}^\infty f(x) \log f(x)\, dx$$

This quantity is called the *differential entropy* of $X$, and it was first considered by Shannon in his paper *A Mathematical Theory of Communication*.  But there are a number of problems with this definition, many of which were first identified by Jaynes (see [here][2], for instance).

## Problems with Differential Entropy

The first problem (which bugged me so much that it motivated me to write this post) is that the definition apparently does not emerge naturally from measure theoretic first principles.  The concept of a surprise function makes sense on an arbitrary probability space, but it is not clear in general how to define a random variable $I$ as in \eqref{entropy} so that its expected value is entropy.

Perhaps a more serious issue is that differential entropy is not coordinate invariant.  Let $X$ be the random variable on $[0,1]$ whose cumulative distribution function is $F(x) = x^2$, meaning $\P(X \leq x) = x^2$ for $x \in [0,1]$.  The density function of this random variable is $F'(x) = 2x$, so its entropy is:

$$H(X) = -\int_0^1 2x \log(2x)\, dx = \frac{1}{2} - \log 2$$

(Note, by the way, that the entropy is negative - it is hard to reconcile this with the interpretation of entropy as "average information"!)  Suppose we apply a coordinate change, such as $u = x^2$; the cumulative distribution function written in the coordinate $u$ is $G(u) = F(\sqrt{u}) = u$, so the density function is $G'(u) = 1$.  It follows that the entropy in this coordinate system is:

$$H(X) = -\int_0^1 1 \log 1\, dx = 0$$

This is a bit of a disaster, and as a result I initially found it almost impossible to believe that differential entropy could be good for much of anything.  But as pointed out in a [mathoverflow question][3] it actually has quite a few applications and is still used quite a lot.  

That said, there is a standard way to "fix" differential entropy, called the *Kullback-Leibler Divergence* or alternatively *relative entropy*.  This quantity depends on two random variables $X$ and $Y$, and it measures the amount of information gained by replacing $X$ with $Y$.  In the continuous setting it has all of the nice properties lacked by differential entropy, including coordinate invariance and positivity, and unlike ordinary entropy it is possible to view the relative entropy of continuous random variables as the limit of discrete counterparts.  So I suspect that relative entropy is the "right" quantity to work with, and I intend to write a follow-up post about it.


[1]: https://archive.org/details/FoundationsOfModernAnalysis_578 "Foundations of Modern Analysis"
[2]: http://bayes.wustl.edu/etj/articles/prior.pdf "Prior Probabilities"
[3]: http://mathoverflow.net/questions/162301/intrinsic-significance-of-differential-entropy "Intrinsic significance of differential entropy"
[4]: {{ site.baseurl }}{% post_url ds/2017-02-12-info-theory-basics %} "Information Theory: The Basics"
