---
layout: post
permalink: /ds/info-theory/neural-entropy/
title: Neural Models and Entropy Optimization
abstract: We show how neural networks are solutions to an information-theoretic variational principle.
date: 2021-02-21
categories: [math, data-science]
comments: true
---

In this post we will study the standard multi-class classification problem, where we are given input vectors $x_1, \ldots, x_n \in \R^d$ and output labels $\ell_1, \ldots, \ell_n$ taking values in a finite set $L = \br{1, \ldots, k}$.
The set of tuples $(x_1, \ell_1), \ldots, (x_n, \ell_n)$ is the _training data_, and our task is to construct a function $f \colon \R^d \to L$ which approximates the training data as well as possible.

With no additional restrictions this task is trivial: define $f(x_i) = \ell_i$ for $i = 1, \ldots, n$, and choose $f(x)$ arbitrarily for all other values of $x$.
But in practice the the data is usually generated by some unknown probability distribution, and our hope is that $f$ will capture enough of this distribution to generalize well to unseen samples.

## Relative entropy minimization

In order to discover strategies for constructing the desired function $f$, we need to start with an empirical principle.
In this post we'll use the _principle of minimum relative entropy_.
Given two probability distributions $p, q \in \R^d$, the relative entropy from $q$ to $p$ is by definition:

$$\KL{p}{q} = -\sum_i p_i \log \frac{q_i}{p_i}$$

(This is often called the _Kullback-Leibler divergence_ after the mathematicians who first studied it.)

The intuitive meaning and theoretical importance of relative entropy can be understood as follows.
Imagine that $q_t$ is a family of probability distributions that we believe model a particular random process.
We sample the outputs of that random process many times, and form the probability distribution $p$ by computing the empirical probabilities.

We can compute the posterior probability distribution $\P(t \vert p)$ over the parameter space from which $t$ is drawn.
It turns out that this is maximized precisely at the parameter $t$ where $\KL{p}{q_t}$ is minimized.
Thus we can view relative entropy as a measure of how well one probability distribution explains or predicts another - the closer it is to zero the more aligned the two distributions are.

## Classifier of minimum relative entropy

Let us now apply the principle of minimum relative entropy to our classification problem.
Our aim is to construct a function $p \colon \R^d \to \R^k$ where $p(x)$ represents the probability distribution of possible labels for $x$.
Thus the $i$th coordinate of $p(x)$, which we will denote by $p(x,i)$, is the probability that the point $x$ has the label $i$.

The assertion that $p(x)$ is a probability distribution imposes two conditions:

- $p(x,i) \geq 0$ for all $x$ and $i$
- $\sum_{i=1}^k p(x,i) = 1$ for all $x$
